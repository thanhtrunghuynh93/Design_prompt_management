{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Tool Use\n",
    "- LangChain provides a straightforward way to bind tools to an LLM, allowing the LLM to decide when to generate a tool call.\n",
    "- For this example, we will define a simple Python function that simulates looking up information (e.g., a fake search tool) and then create a LangChain agent that can use this tool based on user input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# from langchain_openai import ChatOpenAI # Commented out to default to Google\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Uncommented for Google Gemini\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool # Decorator to easily define tools\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor # Components for creating a tool-calling agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embrace the glorious mess that you are.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Using Google GenAI\n",
    "# Verify if API key is present\n",
    "if not os.getenv('GOOGLE_API_KEY'):\n",
    "    raise ValueError(\"Google API key not found. Please check your .env file.\")\n",
    "else:\n",
    "    client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\", contents=\"Write a random small 1 line Thought of the day!\"\n",
    "        # model=\"gemini-2.5-pro-preview-05-06\", contents=\"Write a random small 1 line Thought of the day!\"\n",
    "    )\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized: models/gemini-2.5-pro-preview-05-06\n",
      "\n",
      "AI Response:\n",
      "In brief, **Agentic AI** refers to AI systems that can **proactively and autonomously perceive their environment, make decisions, and take actions to achieve specific goals.**\n",
      "\n",
      "Think of them as AI \"doers\" or \"agents\" that don't just respond to prompts, but can:\n",
      "*   **Set sub-goals**\n",
      "*   **Plan sequences of actions**\n",
      "*   **Use tools (like code interpreters, web browsers, APIs)**\n",
      "*   **Learn from feedback**\n",
      "*   **Operate with a degree of independence**\n",
      "\n",
      "They aim to complete complex tasks with minimal human intervention.\n"
     ]
    }
   ],
   "source": [
    "# Using LangChain Google GenAI\n",
    "# Initialize the language model\n",
    "# A model with function calling capabilities is required for this example.\n",
    "try:\n",
    "    # Example for OpenAI (gpt-4o-mini is a good choice for cost/capability)\n",
    "    # llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) # Commented out\n",
    "    \n",
    "    # Example for Google (uncomment and replace if using Google)\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro-preview-05-06\", temperature=0) # Gemini models support function calling\n",
    "    print(f\"Language model initialized: {llm.model}\")\n",
    "\n",
    "    # Create a simple prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"user\", \"What's Agentic AI in brief?\")\n",
    "    ])\n",
    "    \n",
    "    # Create a simple chain\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Get the response\n",
    "    response = chain.invoke({})\n",
    "    print(\"\\nAI Response:\")\n",
    "    print(response)  \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing language model: {e}\")\n",
    "    print(\"Please ensure your LLM supports function calling and your API key is valid.\")\n",
    "    llm = None # Set llm to None if initialization fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define a Tool ---\n",
    "# We use the @tool decorator to easily define a function as a tool.\n",
    "# The docstring is used as the tool's description, which the LLM sees.\n",
    "# The function signature defines the parameters the tool accepts.\n",
    "\n",
    "@tool\n",
    "def search_information(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches for information on a given query.\n",
    "    Use this tool to find answers to factual questions or get information about topics.\n",
    "    Provides a simulated search result.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Tool Called: search_information with query: '{query}' ---\")\n",
    "    # Simulate searching for information\n",
    "    simulated_results = {\n",
    "        \"weather in London\": \"The weather in London is currently cloudy with a temperature of 15Â°C.\",\n",
    "        \"capital of France\": \"The capital of France is Paris.\",\n",
    "        \"population of Earth\": \"The estimated population of Earth is around 8 billion people.\",\n",
    "        \"tallest mountain\": \"Mount Everest is the tallest mountain above sea level.\",\n",
    "        \"default\": f\"Simulated search result for '{query}': Information found about {query}.\"\n",
    "    }\n",
    "    # Return a simulated result based on the query\n",
    "    result = simulated_results.get(query.lower(), simulated_results[\"default\"])\n",
    "    print(f\"--- Tool Result: {result} ---\")\n",
    "    return result\n",
    "\n",
    "# List of tools available to the agent\n",
    "tools = [search_information]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a Tool-Calling Agent ---\n",
    "# LangChain provides a convenient way to create agents that are capable of using tools.\n",
    "# create_tool_calling_agent is suitable for models with native function calling.\n",
    "\n",
    "# Define the prompt for the agent. It needs a placeholder for tool instructions and chat history.\n",
    "# The `tools_prompt` template is automatically populated by LangChain with tool definitions.\n",
    "# The `agent_scratchpad` is where the agent writes its thoughts and tool calls.\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can answer questions using the available tools.\"),\n",
    "    (\"placeholder\", \"{chat_history}\"), # For conversational memory if needed\n",
    "    (\"human\", \"{input}\"), # The user's current input\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"), # Where the agent plans and calls tools\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Agent with Query: 'What is the tallest mountain?' ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_information` with `{'query': 'tallest mountain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "--- Tool Called: search_information with query: 'tallest mountain' ---\n",
      "--- Tool Result: Mount Everest is the tallest mountain above sea level. ---\n",
      "\u001b[36;1m\u001b[1;3mMount Everest is the tallest mountain above sea level.\u001b[0m\u001b[32;1m\u001b[1;3mMount Everest is the tallest mountain above sea level.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- Final Agent Response ---\n",
      "Mount Everest is the tallest mountain above sea level.\n",
      "\n",
      "--- Running Agent with Query: 'Tell me something about a sausage dog in 2 sentences.' ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_information` with `{'query': 'sausage dog'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "--- Tool Called: search_information with query: 'sausage dog' ---\n",
      "--- Tool Result: Simulated search result for 'sausage dog': Information found about sausage dog. ---\n",
      "\u001b[36;1m\u001b[1;3mSimulated search result for 'sausage dog': Information found about sausage dog.\u001b[0m\u001b[32;1m\u001b[1;3mSausage dogs, also known as Dachshunds, are a breed of dog known for their long bodies and short legs. They were originally bred in Germany to hunt badgers and other den animals.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- Final Agent Response ---\n",
      "Sausage dogs, also known as Dachshunds, are a breed of dog known for their long bodies and short legs. They were originally bred in Germany to hunt badgers and other den animals.\n"
     ]
    }
   ],
   "source": [
    "# Run the example with a test topic\n",
    "# pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "if llm: # Only create agent if LLM initialized successfully\n",
    "    agent = create_tool_calling_agent(llm, tools, agent_prompt)\n",
    "\n",
    "    # Create an AgentExecutor to run the agent.\n",
    "    # AgentExecutor is the runtime that invokes the agent and executes tool calls.\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) # verbose=True shows the agent's thought process\n",
    "\n",
    "\n",
    "    # --- Run the Agent ---\n",
    "    async def run_agent_with_tool(query: str):\n",
    "        \"\"\"Runs the agent with a given query.\"\"\"\n",
    "        print(f\"\\n--- Running Agent with Query: '{query}' ---\")\n",
    "        try:\n",
    "            # Invoke the agent executor asynchronously\n",
    "            response = await agent_executor.ainvoke({\"input\": query, \"chat_history\": []}) # Pass empty chat history for this example\n",
    "            print(\"\\n--- Final Agent Response ---\")\n",
    "            print(response[\"output\"]) # The final answer from the agent\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred during agent execution: {e}\")\n",
    "            print(\"Please ensure your LLM supports function calling and your API key is valid.\")\n",
    "\n",
    "\n",
    "    # Test the agent with queries that should trigger the tool\n",
    "    async def main():\n",
    "        # Run the asynchronous function with different queries\n",
    "        # asyncio.run(run_agent_with_tool(\"What is the capital of France?\"))\n",
    "        # asyncio.run(run_agent_with_tool(\"Tell me about the population of Earth.\"))\n",
    "        # asyncio.run(run_agent_with_tool(\"What's the weather like in London?\"))\n",
    "        asyncio.run(run_agent_with_tool(\"What is the tallest mountain?\"))\n",
    "        asyncio.run(run_agent_with_tool(\"Tell me something about a sausage dog in 2 sentences.\")) # Should use the default simulated result\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping agent execution due to LLM initialization failure.\")\n",
    "\n",
    "# Run the async function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
