{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045e1bef",
   "metadata": {},
   "source": [
    "# 16 - Resource-Aware Optimization\n",
    "- You have an intelligent system designed to answer user questions. \n",
    "- This system first assesses how difficult the question is. For easy questions, it uses a quick, cost-effective language model (like Gemini Flash 1.5). But if the question is complex, the agent considers using a more powerful, albeit more expensive, language model (like Gemini 2.5 Pro). \n",
    "- The decision to use the more powerful model also depends on whether there's enough \"budget\" or time left. The system dynamically makes these choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Python-like structure, not runnable code\n",
    "# In a real-world scenario, you wouldn't just rely on query length. Imagine an advanced setup where an LLM itself acts as the router. This LLM could analyze the query, understand its nuances, and assess its true complexity before deciding which downstream language model is best suited to answer it. For instance, a query asking for a simple factual recall (\"What is the capital of France?\") would be routed to a flash model, while a request requiring deep analysis or creative generation (\"Explain the economic impact of global warming on developing nations\") would be sent to a pro model.\n",
    "\n",
    "from google.adk.agents import Agent\n",
    "\n",
    "# from google.adk.models.lite_llm import LiteLlm # If using models not directly supported by ADK's default Agent\n",
    "\n",
    "# Agent using the more expensive Gemini Pro 2.5\n",
    "gemini_pro_agent = Agent(\n",
    "    name=\"GeminiProAgent\",\n",
    "    model=\"gemini-2.5-pro\",  # Placeholder for actual model name if different\n",
    "    description=\"A highly capable agent for complex queries.\",\n",
    "    instruction=\"You are an expert assistant for complex problem-solving.\",\n",
    ")\n",
    "\n",
    "# Agent using the less expensive Gemini Flash 2.5\n",
    "gemini_flash_agent = Agent(\n",
    "    name=\"GeminiFlashAgent\",\n",
    "    model=\"gemini-2.5-flash\",  # Placeholder for actual model name if different\n",
    "    description=\"A fast and efficient agent for simple queries.\",\n",
    "    instruction=\"You are a quick assistant for straightforward questions.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fee36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Python-like structure, not runnable code\n",
    "\n",
    "from google.adk.agents import Agent, BaseAgent\n",
    "from google.adk.events import Event\n",
    "from google.adk.agents.invocation_context import InvocationContext\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class QueryRouterAgent(BaseAgent):\n",
    "    name: str = \"QueryRouter\"\n",
    "    description: str = (\n",
    "        \"Routes user queries to the appropriate LLM agent based on complexity.\"\n",
    "    )\n",
    "\n",
    "    async def _run_async_impl(\n",
    "        self, context: InvocationContext\n",
    "    ) -> AsyncGenerator[Event, None]:\n",
    "        user_query = context.current_message.text  # Assuming text input\n",
    "        query_length = len(user_query.split())  # Simple metric: number of words\n",
    "\n",
    "        if query_length < 20:  # Example threshold for simplicity vs. complexity\n",
    "            print(\n",
    "                f\"Routing to Gemini Flash Agent for short query (length: {query_length})\"\n",
    "            )\n",
    "            # In a real ADK setup, you would 'transfer_to_agent' or directly invoke\n",
    "            # For demonstration, we'll simulate a call and yield its response\n",
    "            response = await gemini_flash_agent.run_async(context.current_message)\n",
    "            yield Event(author=self.name, content=f\"Flash Agent processed: {response}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Routing to Gemini Pro Agent for long query (length: {query_length})\"\n",
    "            )\n",
    "            response = await gemini_pro_agent.run_async(context.current_message)\n",
    "            yield Event(author=self.name, content=f\"Pro Agent processed: {response}\")\n",
    "\n",
    "\n",
    "CRITIC_SYSTEM_PROMPT = \"\"\"\n",
    "You are the **Critic Agent**, serving as the quality assurance arm of our collaborative research assistant system. Your primary function is to **meticulously review and challenge** information from the Researcher Agent, guaranteeing **accuracy, completeness, and unbiased presentation**.\n",
    "\n",
    "Your duties encompass:\n",
    "* **Assessing research findings** for factual correctness, thoroughness, and potential leanings.\n",
    "* **Identifying any missing data** or inconsistencies in reasoning.\n",
    "* **Raising critical questions** that could refine or expand the current understanding.\n",
    "* **Offering constructive suggestions** for enhancement or exploring different angles.\n",
    "* **Validating that the final output is comprehensive** and balanced.\n",
    "\n",
    "All criticism must be constructive. Your goal is to fortify the research, not invalidate it. Structure your feedback clearly, drawing attention to specific points for revision. Your overarching aim is to ensure the final research product meets the highest possible quality standards.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b575346",
   "metadata": {},
   "source": [
    "## OpenAI-based Resource-Aware Query Router\n",
    "This code demonstrates a resource-aware agent using OpenAI models. It:\n",
    "1. Checks if a prompt is simple (factual), needs reasoning, or needs new information (internet search).\n",
    "2. Uses `gpt-4o-mini` for simple queries, `o4-mini` for reasoning, and for search, calls Google Search API and then passes the result to `gpt-4o-mini`.\n",
    "\n",
    "Prompt used - Using this context here can you write a cod eusing OpenAI which 1. Checks the Prompt and evaluates if its a simple query and then it can be handled using \"gpt-4o-mini\" and its a Prompt which needs Reasoning then \"o4-mini\" and if this needs a new infor or internet search then it first calls google search API, gets result and passes that to \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f35ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing: Using gpt-4o-mini for simple query\n",
      "The capital of France is Paris.\n",
      "-------------------------\n",
      "Routing: Using o4-mini for reasoning query\n",
      "The capital of France is Paris.\n",
      "-------------------------\n",
      "Routing: Using o4-mini for reasoning query\n",
      "\n",
      "-------------------------\n",
      "Routing: Using gpt-4o-mini for simple query\n",
      "\n",
      "-------------------------\n",
      "Routing: Using gpt-4o-mini for simple query\n",
      "As of my last update in October 2023, discussions and developments around AI regulation have been highly active globally. Here are some of the key trends and developments:\n",
      "\n",
      "1. **European Union AI Act**: The EU was in the process of finalizing its AI Act, which aims to establish a comprehensive regulatory framework for AI technologies. This legislation categorizes AI systems based on risk levels and imposes stricter requirements on high-risk applications, particularly those in critical sectors such as healthcare, transportation, and law enforcement.\n",
      "\n",
      "2. **U.S. Initiatives**: In the United States, various federal and state-level discussions were ongoing about AI regulation. The Biden administration released an executive order focused on responsible AI development and use, emphasizing the need for transparency, accountability, and the protection of privacy and civil rights.\n",
      "\n",
      "3. **Global Cooperation**: International bodies like the G7 and OECD were working on principles and frameworks for AI governance, emphasizing the importance of collaboration among countries to address the global challenges posed by AI technologies.\n",
      "\n",
      "4. **Ethical Guidelines**: Various organizations and companies were developing and adopting ethical guidelines for AI deployment, focusing on fairness, accountability, and transparency. There were increasing calls for AI systems to be interpretable and for users to understand how AI decisions are made.\n",
      "\n",
      "5. **Public Engagement**: Governments and institutions were seeking public input on AI guidelines and regulations, recognizing the need for a diverse range of perspectives in crafting effective policies.\n",
      "\n",
      "6. **Focus on Accountability and Security**: There was a strong emphasis on ensuring AI systems are secure and that there are mechanisms for accountability in case of harmful outcomes, including potential biases in AI decision-making.\n",
      "\n",
      "This landscape is rapidly evolving, with ongoing debates and potential new legislation shaping the future of AI regulation. For the most current developments, checking reliable news sources or official government statements would provide the latest updates beyond October 2023.\n",
      "As of my last update in October 2023, discussions and developments around AI regulation have been highly active globally. Here are some of the key trends and developments:\n",
      "\n",
      "1. **European Union AI Act**: The EU was in the process of finalizing its AI Act, which aims to establish a comprehensive regulatory framework for AI technologies. This legislation categorizes AI systems based on risk levels and imposes stricter requirements on high-risk applications, particularly those in critical sectors such as healthcare, transportation, and law enforcement.\n",
      "\n",
      "2. **U.S. Initiatives**: In the United States, various federal and state-level discussions were ongoing about AI regulation. The Biden administration released an executive order focused on responsible AI development and use, emphasizing the need for transparency, accountability, and the protection of privacy and civil rights.\n",
      "\n",
      "3. **Global Cooperation**: International bodies like the G7 and OECD were working on principles and frameworks for AI governance, emphasizing the importance of collaboration among countries to address the global challenges posed by AI technologies.\n",
      "\n",
      "4. **Ethical Guidelines**: Various organizations and companies were developing and adopting ethical guidelines for AI deployment, focusing on fairness, accountability, and transparency. There were increasing calls for AI systems to be interpretable and for users to understand how AI decisions are made.\n",
      "\n",
      "5. **Public Engagement**: Governments and institutions were seeking public input on AI guidelines and regulations, recognizing the need for a diverse range of perspectives in crafting effective policies.\n",
      "\n",
      "6. **Focus on Accountability and Security**: There was a strong emphasis on ensuring AI systems are secure and that there are mechanisms for accountability in case of harmful outcomes, including potential biases in AI decision-making.\n",
      "\n",
      "This landscape is rapidly evolving, with ongoing debates and potential new legislation shaping the future of AI regulation. For the most current developments, checking reliable news sources or official government statements would provide the latest updates beyond October 2023.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise EnvironmentError(\"Please set the GOOGLE_API_KEY environment variable.\")\n",
    "\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "if not GOOGLE_CSE_ID:\n",
    "    raise EnvironmentError(\"Please set the GOOGLE_CSE_ID environment variable.\")\n",
    "\n",
    "\n",
    "def is_simple_query(prompt):\n",
    "    # Heuristic: short, factual, no reasoning words\n",
    "    reasoning_words = [\n",
    "        \"why\",\n",
    "        \"explain\",\n",
    "        \"reason\",\n",
    "        \"analyze\",\n",
    "        \"compare\",\n",
    "        \"how\",\n",
    "        \"impact\",\n",
    "        \"effect\",\n",
    "        \"cause\",\n",
    "    ]\n",
    "    if len(prompt.split()) < 15 and not any(\n",
    "        w in prompt.lower() for w in reasoning_words\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def needs_search(prompt):\n",
    "    # Heuristic: asks for latest, current, or \"search\"/\"find\"\n",
    "    search_words = [\"latest\", \"current\", \"today\", \"search\", \"find\", \"news\"]\n",
    "    return any(w in prompt.lower() for w in search_words)\n",
    "\n",
    "\n",
    "def google_search(query):\n",
    "    loop = (\n",
    "        asyncio.get_event_loop()\n",
    "        if asyncio.get_event_loop().is_running()\n",
    "        else asyncio.new_event_loop()\n",
    "    )\n",
    "\n",
    "    async def fetch():\n",
    "        url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}\"\n",
    "\n",
    "        # Use requests in a thread to avoid blocking event loop\n",
    "        def req():\n",
    "            resp = requests.get(url)\n",
    "            if resp.status_code == 200:\n",
    "                results = resp.json().get(\"items\", [])\n",
    "                return \"\\n\".join([item[\"snippet\"] for item in results[:3]])\n",
    "            return \"No results found.\"\n",
    "\n",
    "        return await asyncio.to_thread(req)\n",
    "\n",
    "    if asyncio.get_event_loop().is_running():\n",
    "        return loop.run_until_complete(fetch())\n",
    "    else:\n",
    "        return asyncio.run(fetch())\n",
    "\n",
    "\n",
    "def ask_openai(prompt, model=\"gpt-4o-mini\"):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_completion_tokens=512,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def route_query(prompt):\n",
    "    if is_simple_query(prompt):\n",
    "        print(\"Routing: Using gpt-4o-mini for simple query\")\n",
    "        return ask_openai(prompt, model=\"gpt-4o-mini\")\n",
    "\n",
    "    elif needs_search(prompt):\n",
    "        print(\"Routing: Using Google Search API + gpt-4o-mini\")\n",
    "        search_result = google_search(prompt)  # Now waits for search to complete\n",
    "        new_prompt = f\"Using this information from Google search, answer the following question as best as possible.\\nSearch Results:\\n{search_result}\\n\\nQuestion: {prompt}\"\n",
    "        return ask_openai(new_prompt, model=\"gpt-4o-mini\")\n",
    "\n",
    "    else:\n",
    "        print(\"Routing: Using o4-mini for reasoning query\")\n",
    "        return ask_openai(prompt, model=\"o4-mini\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "user_prompt = \"What is the capital of France?\"\n",
    "print(route_query(user_prompt))\n",
    "print(\"-------------------------\")\n",
    "\n",
    "user_prompt2 = \"Explain the impact of quantum computing on cryptography.\"\n",
    "print(route_query(user_prompt2))\n",
    "print(\"-------------------------\")\n",
    "\n",
    "user_prompt3 = \"What is the latest news about AI regulation?\"\n",
    "print(route_query(user_prompt3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887f494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
