{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - Prompt Chaining\n",
    "Let's demonstrate a simple prompt chain to take a user's question, rephrase it for better clarity or context for a subsequent step, and then answer the rephrased question using a general-purpose model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_openai import ChatOpenAI # Or import your preferred model like ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always prioritize clear documentation and code readability to enhance collaboration and maintainability in software projects.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify if API key is present\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise ValueError(\"OpenAI API key not found. Please check your .env file.\")\n",
    "\n",
    "# Initialize the OpenAI client and make the API call\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello OpenAI!, Write a random small 1 line Thought of the day!\"}\n",
    "    ]\n",
    ")\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Initialize the language model ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Use the appropriate class and model name for your provider (e.g., ChatGoogleGenerativeAI(model=\"gemini-pro\"))\n",
    "# Setting temperature to control creativity (0.7 is a common balance)\n",
    "try:\n",
    "    # Example for OpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    # Example for Google (uncomment and replace if using Google)\n",
    "    # from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
    "    print(f\"Language model initialized: {llm.model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing language model: {e}\")\n",
    "    print(\"Please ensure your API key is set correctly and the model name is valid.\")\n",
    "    llm = None # Set llm to None if initialization fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Prompt Templates ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the first prompt template: Rephrase the question\n",
    "# This template takes the original user question as input\n",
    "rephrase_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert at rephrasing questions to be clear, concise, and optimized for an AI assistant to answer.\"),\n",
    "    (\"user\", \"Rephrase the following question: {original_question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second prompt template: Answer the rephrased question\n",
    "# This template takes the output from the rephrasing step as input\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Answer the following question based on your knowledge.\"),\n",
    "    (\"user\", \"{rephrased_question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Building the Chain using LangChain Expression Language (LCEL) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['original_question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert at rephrasing questions to be clear, concise, and optimized for an AI assistant to answer.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_question'], input_types={}, partial_variables={}, template='Rephrase the following question: {original_question}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12020c170>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1202206e0>, root_client=<openai.OpenAI object at 0x117ebfd90>, root_async_client=<openai.AsyncOpenAI object at 0x117e83770>, model_name='gpt-4o-mini', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'))] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: The first part of the chain takes the original question,\n",
    "# applies the rephrase_prompt, sends it to the LLM, and parses the output to a string.\n",
    "rephrase_chain = rephrase_prompt | llm | StrOutputParser()\n",
    "print(rephrase_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  rephrased_question: ChatPromptTemplate(input_variables=['original_question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert at rephrasing questions to be clear, concise, and optimized for an AI assistant to answer.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_question'], input_types={}, partial_variables={}, template='Rephrase the following question: {original_question}'), additional_kwargs={})])\n",
      "                      | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12020c170>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1202206e0>, root_client=<openai.OpenAI object at 0x117ebfd90>, root_async_client=<openai.AsyncOpenAI object at 0x117e83770>, model_name='gpt-4o-mini', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "                      | StrOutputParser()\n",
      "} middle=[ChatPromptTemplate(input_variables=['rephrased_question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Answer the following question based on your knowledge.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['rephrased_question'], input_types={}, partial_variables={}, template='{rephrased_question}'), additional_kwargs={})]), ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12020c170>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1202206e0>, root_client=<openai.OpenAI object at 0x117ebfd90>, root_async_client=<openai.AsyncOpenAI object at 0x117e83770>, model_name='gpt-4o-mini', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'))] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Now, chain the output of the rephrase_chain to the answer_prompt\n",
    "# The output of `rephrase_chain` (the rephrased question string) needs to be passed\n",
    "# as the value for the `rephrased_question` variable in the `answer_prompt` template.\n",
    "# LCEL allows us to do this mapping explicitly using a dictionary structure.\n",
    "# The key in the dictionary (\"rephrased_question\") must match the input variable name\n",
    "# expected by the next component (answer_prompt). The value is the preceding chain.\n",
    "full_chain = {\"rephrased_question\": rephrase_chain} | answer_prompt | llm | StrOutputParser()\n",
    "print(full_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Testing the Chain ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question: Tell me about the big statue in Paris people go to see?\n",
      "------------------------------\n",
      "Final answer: The large statue in Paris that attracts many visitors is the Statue of Liberty, specifically the smaller replica located on ÃŽle aux Cygnes in the Seine River. However, the most famous large statue in Paris is the Eiffel Tower, which, while not a statue in the traditional sense, is an iconic landmark that draws millions of tourists each year. If you're referring to a statue, another notable one is the golden statue of Joan of Arc located at Place des Pyramides.\n"
     ]
    }
   ],
   "source": [
    "# --- Testing the Chain ---\n",
    "if llm: # Only run if the model was initialized successfully\n",
    "    original_question = \"Tell me about the big statue in Paris people go to see?\"\n",
    "\n",
    "    print(f\"Original question: {original_question}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    try:\n",
    "        # Invoke the chain with the initial input\n",
    "        response = full_chain.invoke({\"original_question\": original_question})\n",
    "\n",
    "        print(f\"Final answer: {response}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during chain execution: {e}\")\n",
    "        print(\"Please check your API key, model name, and network connection.\")\n",
    "else:\n",
    "    print(\"Chain execution skipped due to LLM initialization failure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question: What is that place in Australia which is degarding due to bad environment?\n",
      "------------------------------\n",
      "Rephrased question: What location in Australia is experiencing degradation due to poor environmental conditions?\n",
      "------------------------------\n",
      "Final answer: One significant location in Australia that is deteriorating due to environmental issues is the Great Barrier Reef. The reef has been severely impacted by climate change, leading to coral bleaching events, ocean acidification, and increased water temperatures. Other factors contributing to its deterioration include pollution from agricultural runoff, coastal development, and overfishing. Efforts are being made to protect and restore the reef, but challenges remain due to ongoing environmental threats.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if llm: # Only run if the model was initialized successfully\n",
    "    # original_question = \"Tell me about the big statue in Paris people go to see?\"\n",
    "    original_question = \"What is that place in Australia which is degarding due to bad environment?\"\n",
    "\n",
    "\n",
    "    print(f\"Original question: {original_question}\")\n",
    "    print(\"-\" * 30)\n",
    "       \n",
    "\n",
    "    try:\n",
    "         # First, get the rephrased question\n",
    "        rephrased = rephrase_chain.invoke({\"original_question\": original_question})\n",
    "        print(f\"Rephrased question: {rephrased}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Invoke the chain with the initial input\n",
    "        # Then get the final answer\n",
    "        response = full_chain.invoke({\"original_question\": original_question})\n",
    "        print(f\"Final answer: {response}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during chain execution: {e}\")\n",
    "        print(\"Please check your API key, model name, and network connection.\")\n",
    "else:\n",
    "    print(\"Chain execution skipped due to LLM initialization failure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Potential Extensions (Code Concepts - Explained in Text) ---\n",
    "# - Adding Intermediate Logic: Insert Python functions using '|' in LCEL.\n",
    "#   Example: rephrase_chain | some_python_function | {\"rephrased_question\": ...} | ...\n",
    "# - Error Handling: Use .with_fallbacks() or try/except blocks around chain invocation.\n",
    "# - State Management with LangGraph: For non-linear or stateful workflows,\n",
    "#   LangGraph provides a more advanced canvas to define nodes and transitions.\n",
    "#   This involves defining a graph state, nodes (LLM calls, functions, tools),\n",
    "#   and edges (transitions, including conditional ones).\n",
    "#   (Full LangGraph example is more complex and would be shown in a dedicated section or chapter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
