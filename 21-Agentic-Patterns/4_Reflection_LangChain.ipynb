{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Reflection\n",
    "- We can demonstrate the core idea of a single reflection step within a LangChain chain using LCEL. This example will show how to generate an initial output, use an LLM to critique.\n",
    "- While full iterative reflection often requires stateful workflows (like LangGraph), a single reflection step can be implemented in LangChain using LCEL to pass output for critique and subsequent refinement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI # Or import your preferred model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized: gpt-4o-mini\n",
      "Thought of the day: \"Every small step forward is a step toward your dreams—keep moving!\"\n"
     ]
    }
   ],
   "source": [
    "## --- Initialize the language model ---\n",
    "# Use the appropriate class and model name for your provider (e.g., ChatGoogleGenerativeAI(model=\"gemini-pro\"))\n",
    "# Setting temperature to control creativity (0.7 is a common balance)\n",
    "try:\n",
    "    # Example for OpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    # Example for Google (uncomment and replace if using Google)\n",
    "    # from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
    "    print(f\"Language model initialized: {llm.model_name}\")\n",
    "\n",
    "    # Create a simple prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"user\", \"Write a simple small Thought of the day!?\")\n",
    "    ])    \n",
    "    # Create a simple chain\n",
    "    chain = prompt | llm | StrOutputParser()    \n",
    "    # Get the response\n",
    "    response = chain.invoke({})\n",
    "    print(\"Thought of the day:\", response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing language model: {e}\")\n",
    "    print(\"Please ensure your API key is set correctly and the model name is valid.\")\n",
    "    llm = None # Set llm to None if initialization fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Initial Content Generation ---\n",
    "# This chain generates the initial piece of content.\n",
    "generation_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", \"Write a short, simple product description for a new smart coffee mug.\"),\n",
    "   (\"user\", \"{product_details}\")\n",
    "])\n",
    "generation_chain = generation_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Reflection / Critique ---\n",
    "# This chain takes the initial generation and critiques it.\n",
    "# The critique prompt asks the LLM to evaluate the generated description.\n",
    "critique_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", \"\"\"Critique the following product description.\n",
    "   Focus on clarity, conciseness, and appeal to potential buyers.\n",
    "   Provide specific suggestions for improvement.\n",
    "   Output your critique and suggestions.\"\"\"),\n",
    "   (\"user\", \"Product Description to Critique:\\n{initial_description}\")\n",
    "])\n",
    "critique_chain = critique_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Refinement ---\n",
    "# This chain takes the original product details AND the critique, and generates a refined description.\n",
    "# We use RunnableParallel to pass both the original details and the critique to the refinement prompt.\n",
    "refinement_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", \"\"\"Based on the original product details and the following critique,\n",
    "   rewrite the product description to make it more effective.\n",
    "   Incorporate the suggestions from the critique.\n",
    "\n",
    "   Original Product Details: {product_details}\n",
    "\n",
    "   Critique: {critique}\n",
    "\n",
    "   Refined Product Description:\"\"\"),\n",
    "   (\"user\", \"\") # User input is empty as all info is in the system prompt context\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pass both initial details and critique to the refinement step, we need to structure the input.\n",
    "# We can use RunnableParallel to create a dictionary containing both.\n",
    "# The input to this parallel block will be the result of the critique step.\n",
    "# We also need the original product_details, so we'll pass the initial input through.\n",
    "# This requires careful chaining to ensure the right information flows to the right places.\n",
    "\n",
    "# A better way to structure this in LCEL for clarity:\n",
    "# Define a chain that takes the initial product_details\n",
    "# This chain will first generate the initial description,\n",
    "# then critique it, and then use the original details + critique for refinement.\n",
    "\n",
    "# Chain: Initial Details -> Generate -> Critique -> Refine\n",
    "# We need to pass the initial details through the critique step to be available for refinement.\n",
    "# We can use RunnableParallel to keep the initial details alongside the critique output.\n",
    "\n",
    "# Chain:\n",
    "# 1. Take initial product_details\n",
    "# 2. Generate initial_description\n",
    "# 3. Pass initial_description to critique_chain\n",
    "# 4. Simultaneously, pass initial product_details through\n",
    "# 5. Combine initial product_details and critique output for refinement_prompt\n",
    "# 6. Run refinement_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Let's build this flow using LCEL:\n",
    "\n",
    "# Step 1 & 2: Generate Initial Description\n",
    "initial_generation_flow = {\n",
    "   \"product_details\": RunnablePassthrough(), # Keep original input\n",
    "   \"initial_description\": generation_chain # Generate initial description\n",
    "}\n",
    "\n",
    "# Step 3 & 4: Critique and Pass Through Original Details\n",
    "# This block takes the output of initial_generation_flow (a dict with product_details and initial_description)\n",
    "# It runs the critique_chain on the initial_description\n",
    "# It also passes the product_details through\n",
    "critique_and_pass_details = RunnableParallel({\n",
    "   \"product_details\": lambda x: x['product_details'], # Pass original details through\n",
    "   \"initial_description\": lambda x: x['initial_description'], # Also pass initial description through\n",
    "   \"critique\": critique_chain # Run the critique chain on the initial description\n",
    "})\n",
    "\n",
    "# Step 5 & 6: Refine based on Original Details and Critique\n",
    "# This block takes the output of critique_and_pass_details (a dict with product_details, initial_description, and critique)\n",
    "# It formats the input for the refinement_prompt\n",
    "refinement_flow = {\n",
    "   \"product_details\": lambda x: x['product_details'],\n",
    "   \"critique\": lambda x: x['critique'],\n",
    "   \"initial_description\": lambda x: x['initial_description'] # Pass through if needed for context in prompt, though not strictly required by refinement_prompt here\n",
    "} | refinement_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# Combine the steps into a single chain\n",
    "# Initial input (product_details) -> initial_generation_flow -> critique_and_pass_details -> refinement_flow\n",
    "full_reflection_chain = initial_generation_flow | critique_and_pass_details | refinement_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code will run as a Python program,but will give error in Jupyter notebook. \n",
    "## The correct Jupyter Notebook equivalent is in the cell below.\n",
    "\n",
    "# # --- Run the Chain ---\n",
    "# async def run_reflection_example(product_details: str):\n",
    "#    \"\"\"Runs the LangChain reflection example with product details.\"\"\"\n",
    "#    if not llm:\n",
    "#        print(\"LLM not initialized. Cannot run example.\")\n",
    "#        return\n",
    "\n",
    "#    print(f\"\\n--- Running Reflection Example for Product: '{product_details}' ---\")\n",
    "#    # Invoke the chain asynchronously\n",
    "#    try:\n",
    "#        final_refined_description = await full_reflection_chain.ainvoke(product_details)\n",
    "#        print(\"\\n--- Final Refined Product Description ---\")\n",
    "#        print(final_refined_description)\n",
    "#    except Exception as e:\n",
    "#        print(f\"\\nAn error occurred during chain execution: {e}\")\n",
    "\n",
    "# # Run the example with test product details\n",
    "# if __name__ == \"__main__\":\n",
    "#    test_product_details = \"A mug that keeps coffee hot and can be controlled by a smartphone app.\"\n",
    "#    # Run the asynchronous function\n",
    "#    asyncio.run(run_reflection_example(test_product_details))\n",
    "\n",
    "#    test_product_details_2 = \"A durable backpack made from recycled materials with many pockets.\"\n",
    "#    asyncio.run(run_reflection_example(test_product_details_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Reflection Example for Product: 'A pen which helps you write in a beautiful handwriting.' ---\n",
      "\n",
      "--- Final Refined Product Description ---\n",
      "Revised Product Description for the Handwriting Pen:\n",
      "\n",
      "\"Transform your writing experience with our Elegant Handwriting Pen! Designed for those who desire beautiful handwriting, this pen seamlessly combines style and functionality. Its fine-tip nib allows for precise control, enabling you to create stunning letters and elegant script effortlessly.\n",
      "\n",
      "Say goodbye to messy notes and frustrating scribbles—this pen glides smoothly across the page, making every word a joy to write. Whether you're journaling, addressing invitations, or simply expressing your thoughts, you'll relish the art of writing like never before.\n",
      "\n",
      "Crafted with a sleek, ergonomic design, our pen fits comfortably in your hand, reducing fatigue during extended writing sessions. Plus, its vibrant ink dries quickly to prevent smudging, ensuring your work looks pristine.\n",
      "\n",
      "Embrace the beauty of your handwriting and elevate your writing tasks. Don't miss out on the chance to bring elegance to your everyday writing. Order your Elegant Handwriting Pen today and rediscover the joy of putting pen to paper!\"\n"
     ]
    }
   ],
   "source": [
    "# Run the example with a test topic\n",
    "# pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def run_reflection_example(product_details: str):\n",
    "   \"\"\"Runs the LangChain reflection example with product details.\"\"\"\n",
    "   if not llm:\n",
    "       print(\"LLM not initialized. Cannot run example.\")\n",
    "       return\n",
    "\n",
    "   print(f\"\\n--- Running Reflection Example for Product: '{product_details}' ---\")\n",
    "   # Invoke the chain asynchronously\n",
    "   try:\n",
    "       final_refined_description = await full_reflection_chain.ainvoke(product_details)\n",
    "       print(\"\\n--- Final Refined Product Description ---\")\n",
    "       print(final_refined_description)\n",
    "   except Exception as e:\n",
    "       print(f\"\\nAn error occurred during chain execution: {e}\")\n",
    "\n",
    "# Then modify the running code to use await\n",
    "async def main():\n",
    "   test_product_details = \"A mug that keeps coffee hot and can be controlled by a smartphone app.\"\n",
    "   test_product_details = \"A pen which helps you write in a beautiful handwriting.\"\n",
    "   asyncio.run(run_reflection_example(test_product_details))\n",
    "\n",
    "   # test_product_details_2 = \"A durable backpack made from recycled materials with many pockets.\"\n",
    "   #asyncio.run(run_reflection_example(test_product_details_2))\n",
    "\n",
    "# Run the async function\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
