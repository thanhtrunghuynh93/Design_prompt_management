{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Parallelization \n",
    "For this example, we will create a simple LangChain chain that takes a user query and simultaneously performs two independent \"tasks\" (simulated by simple chains or functions) before combining their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI # Or import your preferred model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Set your API key - replace with your actual environment variable or key management\n",
    "# Ensure you have set the appropriate environment variable (e.g., OPENAI_API_KEY, GOOGLE_API_KEY)\n",
    "# Example (replace with your actual method for managing API keys):\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_ACTUAL_OPENAI_API_KEY\"\n",
    "# Or for Google:\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_ACTUAL_GOOGLE_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "## --- Initialize the language model ---\n",
    "# Use the appropriate class and model name for your provider (e.g., ChatGoogleGenerativeAI(model=\"gemini-pro\"))\n",
    "# Setting temperature to control creativity (0.7 is a common balance)\n",
    "try:\n",
    "    # Example for OpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    # Example for Google (uncomment and replace if using Google)\n",
    "    # from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
    "    print(f\"Language model initialized: {llm.model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing language model: {e}\")\n",
    "    print(\"Please ensure your API key is set correctly and the model name is valid.\")\n",
    "    llm = None # Set llm to None if initialization fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Independent Tasks (Simulated Workflows) ---\n",
    "# These represent tasks that can be executed in parallel.\n",
    "# In a real scenario, these might be calls to different APIs, tools, or separate chains.\n",
    "\n",
    "# Task 1: Summarize the topic\n",
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", \"Summarize the following topic concisely:\"),\n",
    "   (\"user\", \"{topic}\")\n",
    "])\n",
    "summarize_chain = summarize_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Task 2: Generate related questions\n",
    "questions_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", \"Generate three interesting questions about the following topic:\"),\n",
    "   (\"user\", \"{topic}\")\n",
    "])\n",
    "questions_chain = questions_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Task 3: Identify key terms\n",
    "terms_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", \"Identify 5-10 key terms from the following topic, separated by commas:\"),\n",
    "   (\"user\", \"{topic}\")\n",
    "])\n",
    "terms_chain = terms_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine Tasks for Parallel Execution using RunnableParallel ---\n",
    "# RunnableParallel allows you to run multiple runnables in parallel.\n",
    "# It takes a dictionary where keys are the output keys and values are the runnables to run.\n",
    "# The input to RunnableParallel is passed to each runnable in the dictionary.\n",
    "# The output is a dictionary containing the results of each runnable, keyed by the dictionary keys.\n",
    "\n",
    "# The input to this parallel block will be the user's original query (topic).\n",
    "# We use RunnablePassthrough() to pass the input directly to the parallel runnables.\n",
    "parallel_tasks = RunnableParallel({\n",
    "   \"summary\": summarize_chain,\n",
    "   \"questions\": questions_chain,\n",
    "   \"key_terms\": terms_chain\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a chain that combines the original input with parallel results ---\n",
    "# This approach uses a dictionary that merges inputs and outputs correctly\n",
    "def combine_inputs_and_parallel_results(inputs):\n",
    "   # inputs here is just the topic string\n",
    "   return {\n",
    "       \"topic\": inputs,  # Keep the original topic\n",
    "       # The rest will be filled by parallel_tasks\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define a Final Synthesis Step ---\n",
    "# This step takes the results from the parallel tasks and combines them into a final answer.\n",
    "# It requires the parallel tasks to complete before it can run.\n",
    "synthesis_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", \"\"\"Based on the following information about a topic:\n",
    "\n",
    "   Summary: {summary}\n",
    "\n",
    "   Related Questions: {questions}\n",
    "\n",
    "   Key Terms: {key_terms}\n",
    "\n",
    "   Synthesize a comprehensive answer that includes the summary, lists the related questions, and mentions the key terms.\"\"\"),\n",
    "   (\"user\", \"Original topic: {topic}\") # Include the original topic for context\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build the Full Chain ---\n",
    "# First, we run the parallel tasks\n",
    "# Then, we combine the results with the original topic\n",
    "# Finally, we use that combined data for synthesis\n",
    "full_parallel_chain = (\n",
    "   RunnableParallel({\n",
    "       \"topic\": RunnablePassthrough(),  # Pass through the original topic\n",
    "       \"parallel_results\": parallel_tasks  # Run all parallel tasks\n",
    "   })\n",
    "   .assign(  # Restructure the data for the synthesis prompt\n",
    "       summary=lambda x: x[\"parallel_results\"][\"summary\"],\n",
    "       questions=lambda x: x[\"parallel_results\"][\"questions\"],\n",
    "       key_terms=lambda x: x[\"parallel_results\"][\"key_terms\"]\n",
    "   )\n",
    "   | synthesis_prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the Chain ---\n",
    "async def run_parallel_example(topic: str):\n",
    "   \"\"\"Runs the parallel LangChain example with a given topic.\"\"\"\n",
    "   if not llm:\n",
    "       print(\"LLM not initialized. Cannot run example.\")\n",
    "       return\n",
    "\n",
    "   print(f\"\\n--- Running Parallel LangChain Example for Topic: '{topic}' ---\")\n",
    "   # Invoke the chain asynchronously\n",
    "   try:\n",
    "       response = await full_parallel_chain.ainvoke(topic)\n",
    "       print(\"\\n--- Final Response ---\")\n",
    "       print(response)\n",
    "   except Exception as e:\n",
    "       print(f\"\\nAn error occurred during chain execution: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Parallel LangChain Example for Topic: 'Artificial Intelligence' ---\n",
      "\n",
      "--- Final Response ---\n",
      "**Summary**: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn. It encompasses various technologies, including machine learning, natural language processing, and robotics, and is used in applications ranging from virtual assistants to autonomous vehicles. AI aims to enhance efficiency, automate tasks, and provide insights from data, but it also raises ethical concerns regarding privacy, bias, and job displacement.\n",
      "\n",
      "**Related Questions**:\n",
      "1. How has the integration of artificial intelligence into various industries evolved since 2023, and what are the most significant advancements that have emerged in the last few years?\n",
      "2. In what ways are ethical considerations surrounding artificial intelligence, including bias and accountability, being addressed by researchers and policymakers as we move further into the 2020s?\n",
      "3. How are the public perceptions of artificial intelligence changing post-2023, particularly in regard to its impact on employment, privacy, and societal norms?\n",
      "\n",
      "**Key Terms**: Machine Learning, Neural Networks, Deep Learning, Natural Language Processing, Computer Vision, Algorithm, Data Mining, Automation, Robotics, Predictive Analytics.\n",
      "\n",
      "As AI continues to evolve, it has become increasingly integrated into various sectors, demonstrating significant advancements in areas such as machine learning and neural networks. The ongoing discourse around ethical considerations highlights the importance of addressing biases and ensuring accountability in AI systems. Furthermore, public perceptions of AI are shifting, particularly concerning its implications for employment, privacy, and societal norms, as we navigate further into the 2020s.\n",
      "\n",
      "--- Running Parallel LangChain Example for Topic: 'Renewable Energy' ---\n",
      "\n",
      "--- Final Response ---\n",
      "**Renewable Energy: A Comprehensive Overview**\n",
      "\n",
      "**Summary:**  \n",
      "Renewable energy refers to energy sources that are naturally replenished and sustainable, including solar, wind, hydro, geothermal, and biomass. These energy sources play a crucial role in reducing greenhouse gas emissions, decreasing reliance on fossil fuels, and promoting environmental sustainability. The transition to renewable energy is vital not only for combating climate change but also for enhancing energy security and fostering economic growth through the development of green technologies and job creation.\n",
      "\n",
      "**Related Questions:**  \n",
      "1. How have advancements in renewable energy technology, such as solar panels and wind turbines, influenced global energy policies and economic growth in the last decade?  \n",
      "2. What are the most promising emerging renewable energy sources that could significantly contribute to reducing global carbon emissions by 2030?  \n",
      "3. In what ways can governments and businesses collaborate to overcome the challenges of integrating renewable energy into existing energy grids and ensuring energy accessibility for all?\n",
      "\n",
      "**Key Terms:**  \n",
      "- Sustainability  \n",
      "- Solar Power  \n",
      "- Wind Energy  \n",
      "- Biomass  \n",
      "- Hydropower  \n",
      "- Geothermal  \n",
      "- Energy Efficiency  \n",
      "- Clean Technology  \n",
      "- Carbon Footprint  \n",
      "- Energy Transition  \n",
      "\n",
      "In summary, renewable energy is essential for a sustainable future, and understanding its impact on policy, emerging technologies, and the collaboration between governments and businesses is crucial for maximizing its benefits.\n"
     ]
    }
   ],
   "source": [
    "# Run the example with a test topic\n",
    "# pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Then modify the running code to use await\n",
    "async def main():\n",
    "    test_topic = \"Artificial Intelligence\"\n",
    "    await run_parallel_example(test_topic)\n",
    "    \n",
    "    test_topic_2 = \"Renewable Energy\"\n",
    "    await run_parallel_example(test_topic_2)\n",
    "\n",
    "# Run the async function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
