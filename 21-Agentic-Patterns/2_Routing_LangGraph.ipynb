{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Routing\n",
    "For this example, we will use LangGraph to build a simple agent that routes user queries to one of two different \"tools\" (represented by simple functions) based on the user's input. This demonstrates LLM-based routing within a stateful workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal # Used for type hinting specific string values\n",
    "from langchain_openai import ChatOpenAI # Or import your preferred model like ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser # To parse LLM output to string\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field # For defining the graph state\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, END # Core LangGraph components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Embrace the uncertainty of today; it may lead to the magic of tomorrow.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify if API key is present\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise ValueError(\"OpenAI API key not found. Please check your .env file.\")\n",
    "\n",
    "# Initialize the OpenAI client and make the API call\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello OpenAI!, Write a random small 1 line Thought of the day!\"}\n",
    "    ]\n",
    ")\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Initialize the language model ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Use the appropriate class and model name for your provider (e.g., ChatGoogleGenerativeAI(model=\"gemini-pro\"))\n",
    "# Setting temperature to control creativity (0.7 is a common balance)\n",
    "try:\n",
    "    # Example for OpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    # Example for Google (uncomment and replace if using Google)\n",
    "    # from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
    "    print(f\"Language model initialized: {llm.model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing language model: {e}\")\n",
    "    print(\"Please ensure your API key is set correctly and the model name is valid.\")\n",
    "    llm = None # Set llm to None if initialization fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Define the Graph State ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The state object that will be passed between nodes in the graph.\n",
    "# It holds all the information needed for routing and processing throughout the workflow.\n",
    "class GraphState(BaseModel):\n",
    "   \"\"\"Represents the state of our graph.\"\"\"\n",
    "   # The user's original question that initiates the workflow\n",
    "   question: str = Field(description=\"The user's original question.\")\n",
    "   # The chosen route based on the analysis of the question. Literal ensures type safety.\n",
    "   route: Literal[\"tool_a\", \"tool_b\", \"unclear\"] | None = Field(\n",
    "       description=\"The chosen route based on question analysis.\",\n",
    "       default=None # Default value is None before routing decision is made\n",
    "   )\n",
    "   # Placeholder for output from Tool A (if executed)\n",
    "   tool_a_output: str | None = Field(\n",
    "       description=\"Output from Tool A.\",\n",
    "       default=None\n",
    "   )\n",
    "   # Placeholder for output from Tool B (if executed)\n",
    "   tool_b_output: str | None = Field(\n",
    "       description=\"Output from Tool B.\",\n",
    "       default=None\n",
    "   )\n",
    "   # The final answer generated for the user\n",
    "   final_answer: str | None = Field(\n",
    "       description=\"The final answer to the user's question.\",\n",
    "       default=None\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Define the Nodes (Steps in the Workflow) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Route the question\n",
    "# This node's responsibility is to analyze the input question and decide the next path.\n",
    "def route_question(state: GraphState) -> GraphState:\n",
    "   \"\"\"\n",
    "   Analyzes the question and decides which tool to route to.\n",
    "   Uses an LLM call for routing logic.\n",
    "   \"\"\"\n",
    "   print(\"\\n---ROUTE QUESTION---\") # Print statement to show node execution\n",
    "   question = state.question\n",
    "\n",
    "\n",
    "   # Prompt the LLM to decide the route\n",
    "   # The system prompt clearly defines the task and expected output format.\n",
    "   route_prompt = ChatPromptTemplate.from_messages([\n",
    "       (\"system\", \"\"\"Analyze the user's question and determine if it is related to 'Tool A' or 'Tool B'.\n",
    "        Tool A is related to processing data or performing calculations.\n",
    "        Tool B is related to retrieving information or providing summaries.\n",
    "        Output 'tool_a' if the question is primarily about Tool A.\n",
    "        Output 'tool_b' if the question is primarily about Tool B.\n",
    "        Output 'unclear' if the question is not clearly related to either, is ambiguous, or asks for something else.\n",
    "        ONLY output one of the following words: tool_a, tool_b, unclear\"\"\"),\n",
    "       (\"user\", \"{question}\") # The user's question is the input to this prompt\n",
    "   ])\n",
    "\n",
    "\n",
    "   # Create a chain for routing: Prompt -> LLM -> Parse Output\n",
    "   route_chain = route_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "   # Invoke the routing chain to get the decision\n",
    "   try:\n",
    "       route_decision = route_chain.invoke({\"question\": question})\n",
    "       print(f\"LLM Routing decision raw output: '{route_decision}'\")\n",
    "       # Update the state with the routing decision after cleaning up whitespace and converting to lowercase\n",
    "       state.route = route_decision.strip().lower()\n",
    "       print(f\"State updated with route: {state.route}\")\n",
    "   except Exception as e:\n",
    "       print(f\"Error during routing LLM call: {e}\")\n",
    "       state.route = \"unclear\" # Default to unclear on error\n",
    "       print(f\"Error occurred, defaulting route to: {state.route}\")\n",
    "\n",
    "   return state # Return the updated state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 2: Execute Tool A\n",
    "# This node simulates the action of using Tool A.\n",
    "def execute_tool_a(state: GraphState) -> GraphState:\n",
    "   \"\"\"\n",
    "   Simulates executing Tool A and updates the state with its output.\n",
    "   \"\"\"\n",
    "   print(\"\\n---EXECUTE TOOL A---\")\n",
    "   question = state.question\n",
    "   # In a real application, this would contain the actual code to call an external tool/API\n",
    "   # For simulation, we just generate a string based on the input question.\n",
    "   tool_output = f\"Tool A processed the question: '{question}'. Result: Simulated data processing output from Tool A.\"\n",
    "   print(f\"Simulated Tool A Output: {tool_output}\")\n",
    "   state.tool_a_output = tool_output # Store the tool's output in the state\n",
    "   return state # Return the updated state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 3: Execute Tool B\n",
    "# This node simulates the action of using Tool B.\n",
    "def execute_tool_b(state: GraphState) -> GraphState:\n",
    "   \"\"\"\n",
    "   Simulates executing Tool B and updates the state with its output.\n",
    "   \"\"\"\n",
    "   print(\"\\n---EXECUTE TOOL B---\")\n",
    "   question = state.question\n",
    "   # In a real application, this would contain the actual code to call a different external tool/API\n",
    "   # For simulation, we generate a different string based on the input question.\n",
    "   tool_output = f\"Tool B processed the question: '{question}'. Result: Simulated information retrieval output from Tool B.\"\n",
    "   print(f\"Simulated Tool B Output: {tool_output}\")\n",
    "   state.tool_b_output = tool_output # Store the tool's output in the state\n",
    "   return state # Return the updated state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 4: Handle Unclear Questions\n",
    "# This node provides a specific response when the routing is unclear.\n",
    "def handle_unclear(state: GraphState) -> GraphState:\n",
    "   \"\"\"\n",
    "   Generates a response for unclear questions and sets the final answer.\n",
    "   \"\"\"\n",
    "   print(\"\\n---HANDLE UNCLEAR---\")\n",
    "   question = state.question\n",
    "   response = f\"I'm sorry, I couldn't determine how to process your question: '{question}'. Could you please rephrase it? I can help with data processing (Tool A) or information retrieval (Tool B).\"\n",
    "   print(f\"Unclear Response Generated: {response}\")\n",
    "   state.final_answer = response # Set the final answer in the state\n",
    "   return state # Return the updated state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 5: Synthesize Final Answer (after tool execution)\n",
    "# This node takes the output from a tool and formats the final response for the user.\n",
    "def synthesize_answer(state: GraphState) -> GraphState:\n",
    "   \"\"\"\n",
    "   Synthesizes the final answer based on tool outputs stored in the state.\n",
    "   \"\"\"\n",
    "   print(\"\\n---SYNTHESIZE ANSWER---\")\n",
    "   question = state.question\n",
    "   # Check which tool output is available in the state\n",
    "   if state.tool_a_output:\n",
    "       final_answer = f\"Regarding your question: '{question}', here is the result from Tool A: {state.tool_a_output}\"\n",
    "   elif state.tool_b_output:\n",
    "        final_answer = f\"Regarding your question: '{question}', here is the result from Tool B: {state.tool_b_output}\"\n",
    "   else:\n",
    "       # This case should ideally not be reached if routing works correctly and leads to a tool,\n",
    "       # but included as a fallback for robustness.\n",
    "       print(\"Warning: Synthesize node reached without tool output.\")\n",
    "       final_answer = f\"Could not fully process your question: '{question}'.\"\n",
    "\n",
    "\n",
    "   print(f\"Synthesized Final Answer: {final_answer}\")\n",
    "   state.final_answer = final_answer # Set the final answer in the state\n",
    "   return state # Return the updated state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Define the Conditional Edge (Routing Logic) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is called by LangGraph after the 'route_question' node executes.\n",
    "# It determines the next node based on the 'state.route' value.\n",
    "def route_decision_edge(state: GraphState) -> Literal[\"tool_a\", \"tool_b\", \"unclear\", \"synthesize\"]:\n",
    "   \"\"\"\n",
    "   Conditional edge that routes based on the 'route' state variable.\n",
    "   Returns the name of the next node to transition to.\n",
    "   \"\"\"\n",
    "   print(f\"\\n---ROUTING EDGE DECISION BASED ON STATE.ROUTE: '{state.route}'---\")\n",
    "   # LangGraph will use this return value to follow the corresponding edge.\n",
    "   if state.route == \"tool_a\":\n",
    "       return \"tool_a\" # Go to the execute_tool_a node\n",
    "   elif state.route == \"tool_b\":\n",
    "       return \"tool_b\" # Go to the execute_tool_b node\n",
    "   elif state.route == \"unclear\":\n",
    "       return \"unclear\" # Go to the handle_unclear node\n",
    "   else:\n",
    "       # Default or fallback if route value is unexpected (should be caught by Literal type hint, but good practice)\n",
    "       print(f\"Warning: Unexpected route decision '{state.route}', defaulting to unclear.\")\n",
    "       return \"unclear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running with a question intended for Tool A (Data Processing) ---\n",
      "\n",
      "---ROUTE QUESTION---\n",
      "LLM Routing decision raw output: 'tool_a'\n",
      "State updated with route: tool_a\n",
      "\n",
      "---ROUTING EDGE DECISION BASED ON STATE.ROUTE: 'tool_a'---\n",
      "\n",
      "---EXECUTE TOOL A---\n",
      "Simulated Tool A Output: Tool A processed the question: 'Can you help me with some data processing?'. Result: Simulated data processing output from Tool A.\n",
      "\n",
      "---SYNTHESIZE ANSWER---\n",
      "Synthesized Final Answer: Regarding your question: 'Can you help me with some data processing?', here is the result from Tool A: Tool A processed the question: 'Can you help me with some data processing?'. Result: Simulated data processing output from Tool A.\n",
      "Final Output A: Regarding your question: 'Can you help me with some data processing?', here is the result from Tool A: Tool A processed the question: 'Can you help me with some data processing?'. Result: Simulated data processing output from Tool A.\n",
      "\n",
      "--- Running with an Unclear question ---\n",
      "\n",
      "---ROUTE QUESTION---\n",
      "LLM Routing decision raw output: 'unclear'\n",
      "State updated with route: unclear\n",
      "\n",
      "---ROUTING EDGE DECISION BASED ON STATE.ROUTE: 'unclear'---\n",
      "\n",
      "---HANDLE UNCLEAR---\n",
      "Unclear Response Generated: I'm sorry, I couldn't determine how to process your question: 'Tell me a story about a dragon.'. Could you please rephrase it? I can help with data processing (Tool A) or information retrieval (Tool B).\n",
      "Final Output C: I'm sorry, I couldn't determine how to process your question: 'Tell me a story about a dragon.'. Could you please rephrase it? I can help with data processing (Tool A) or information retrieval (Tool B).\n",
      "\n",
      "--- Running with Another Unclear question ---\n",
      "\n",
      "---ROUTE QUESTION---\n",
      "LLM Routing decision raw output: 'tool_b'\n",
      "State updated with route: tool_b\n",
      "\n",
      "---ROUTING EDGE DECISION BASED ON STATE.ROUTE: 'tool_b'---\n",
      "\n",
      "---EXECUTE TOOL B---\n",
      "Simulated Tool B Output: Tool B processed the question: 'What's the weather like tomorrow?'. Result: Simulated information retrieval output from Tool B.\n",
      "\n",
      "---SYNTHESIZE ANSWER---\n",
      "Synthesized Final Answer: Regarding your question: 'What's the weather like tomorrow?', here is the result from Tool B: Tool B processed the question: 'What's the weather like tomorrow?'. Result: Simulated information retrieval output from Tool B.\n",
      "Final Output D: Regarding your question: 'What's the weather like tomorrow?', here is the result from Tool B: Tool B processed the question: 'What's the weather like tomorrow?'. Result: Simulated information retrieval output from Tool B.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the LangGraph ---\n",
    "if llm: # Only build graph if LLM initialized successfully\n",
    "   workflow = StateGraph(GraphState) # Initialize the graph with the state definition\n",
    "\n",
    "\n",
    "   # Add nodes to the graph workflow\n",
    "   workflow.add_node(\"route_question\", route_question)\n",
    "   workflow.add_node(\"execute_tool_a\", execute_tool_a)\n",
    "   workflow.add_node(\"execute_tool_b\", execute_tool_b)\n",
    "   workflow.add_node(\"handle_unclear\", handle_unclear)\n",
    "   workflow.add_node(\"synthesize_answer\", synthesize_answer)\n",
    "\n",
    "\n",
    "   # Set the entry point - where the graph execution begins\n",
    "   workflow.set_entry_point(\"route_question\")\n",
    "\n",
    "\n",
    "   # Add edges to define the transitions between nodes\n",
    "\n",
    "\n",
    "   # The 'route_question' node's output determines the next step via a conditional edge\n",
    "   workflow.add_conditional_edges(\n",
    "       \"route_question\", # Source node: Execution comes from 'route_question'\n",
    "       route_decision_edge, # The function that decides the next node based on state\n",
    "       {\n",
    "           \"tool_a\": \"execute_tool_a\", # If route_decision_edge returns \"tool_a\", transition to 'execute_tool_a' node\n",
    "           \"tool_b\": \"execute_tool_b\", # If route_decision_edge returns \"tool_b\", transition to 'execute_tool_b' node\n",
    "           \"unclear\": \"handle_unclear\", # If route_decision_edge returns \"unclear\", transition to 'handle_unclear' node\n",
    "       }\n",
    "   )\n",
    "\n",
    "\n",
    "   # After executing Tool A or Tool B, the workflow goes to synthesize the answer (unconditional edges)\n",
    "   workflow.add_edge(\"execute_tool_a\", \"synthesize_answer\")\n",
    "   workflow.add_edge(\"execute_tool_b\", \"synthesize_answer\")\n",
    "\n",
    "\n",
    "   # The 'handle_unclear' node is a terminal node - the workflow ends here\n",
    "   workflow.add_edge(\"handle_unclear\", END)\n",
    "\n",
    "\n",
    "   # The 'synthesize_answer' node is also a terminal node - the workflow ends here\n",
    "   workflow.add_edge(\"synthesize_answer\", END)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   # Compile the graph into a runnable application\n",
    "   app = workflow.compile()\n",
    "\n",
    "\n",
    "   # --- Run the Graph with different questions ---\n",
    "   # We will test the routing with questions intended for different tools and an unclear question.\n",
    "\n",
    "   print(\"\\n--- Running with a question intended for Tool A (Data Processing) ---\")\n",
    "   inputs_a = {\"question\": \"Can you help me with some data processing?\"} # Example question for Tool A\n",
    "   output_a = app.invoke(inputs_a) # Invoke the graph with the input\n",
    "   print(f\"Final Output A: {output_a['final_answer']}\") # Print the final answer from the state\n",
    "\n",
    "\n",
    "   print(\"\\n--- Running with an Unclear question ---\")\n",
    "   inputs_c = {\"question\": \"Tell me a story about a dragon.\"} # Example unclear question\n",
    "   output_c = app.invoke(inputs_c) # Invoke the graph with the input\n",
    "   print(f\"Final Output C: {output_c['final_answer']}\") # Print the final answer from the state\n",
    "\n",
    "\n",
    "   print(\"\\n--- Running with Another Unclear question ---\")\n",
    "   inputs_d = {\"question\": \"What's the weather like tomorrow?\"} # Another example unclear question\n",
    "   output_d = app.invoke(inputs_d) # Invoke the graph with the input\n",
    "   print(f\"Final Output D: {output_d['final_answer']}\") # Print the final answer from the state\n",
    "\n",
    "\n",
    "else:\n",
    "   print(\"\\nSkipping LangGraph execution due to LLM initialization failure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
